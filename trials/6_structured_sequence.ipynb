{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 6: structured sequence modeling\n",
    "\n",
    "* Create simple parametric time series and try to model them.\n",
    "* Add structure by constructing a graph between the series and see how it improves.\n",
    "* Usage of `tflearn` inspired by [How to do time series prediction using RNNs, TensorFlow and Cloud ML Engine](https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('..', 'data', 'structured_sequence_trial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100\n",
    "N_SEQ = 4\n",
    "\n",
    "def create_time_series(seq_len, random_state):\n",
    "    freq = random_state.uniform(0.1, 0.6)\n",
    "    ampl = random_state.uniform(0.5, 1.5)\n",
    "    offset = random_state.uniform(-1, 1)\n",
    "    return np.sin(np.arange(seq_len) * freq) * ampl + offset\n",
    "\n",
    "rs = np.random.RandomState(42)\n",
    "data = np.empty((N_SEQ, SEQ_LEN))\n",
    "for i in range(N_SEQ):\n",
    "    data[i] = create_time_series(SEQ_LEN, rs)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.T.plot();\n",
    "plt.savefig('time_series.pdf')\n",
    "# hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Graph construction\n",
    "\n",
    "k-NN graph between the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data preparation\n",
    "\n",
    "* Store data in TFRecords files which will be read by the input pipeline.\n",
    "* Preprocessing can be done here.\n",
    "* Data augmentation should be done in input pipeline (to save disk space).\n",
    "* We are doing full batch, i.e. we feed data on the whole graph at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUTS = 10  # Number of samples used for prediction, i.e. unrolling length.\n",
    "N_OUTPUTS = 1  # Number of samples in the time series the model tries to predict.\n",
    "\n",
    "def feature(array):\n",
    "    array = array.reshape(-1)\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list(array)))\n",
    "\n",
    "def save_dataset(data, filename):\n",
    "    \"\"\"Save dataset as TFRecords.\"\"\"\n",
    "    filename = os.path.join(DATA_DIR, filename)\n",
    "    num_examples = data.shape[1] - N_INPUTS - N_OUTPUTS + 1\n",
    "    assert num_examples > 0\n",
    "    tf.logging.info('Writing {} examples to {}'.format(num_examples, filename))\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for idx in range(num_examples):\n",
    "            inputs = data[:, idx:idx+N_INPUTS]\n",
    "            targets = data[:, idx+N_INPUTS:idx+N_INPUTS+N_OUTPUTS]\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                #'graph': feature(graph),  # Adjacency matrix or Laplacian can be stored here.\n",
    "                'inputs': feature(inputs),\n",
    "                'targets': feature(targets)}))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "TRAINING_LEN = int(0.8 * SEQ_LEN)\n",
    "save_dataset(data.iloc[:, :TRAINING_LEN].values, 'train.tfrecords')\n",
    "save_dataset(data.iloc[:, TRAINING_LEN:].values, 'validation.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data loading\n",
    "\n",
    "Two training schemes:\n",
    "* Load whole data for training up to a certain point in time. That is what is done for text (the whole vocabulary graph is used).\n",
    "* Use some time series (some part of the graph) as training and the others as evaluation.\n",
    "\n",
    "TF alternative:\n",
    "* [tf.contrib.slim.dataset](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(s, filenames, num_epochs=1, batch_size=1, read_threads=1, seed=None):\n",
    "        #if mode == tflearn.ModeKeys.TRAIN:\n",
    "        s.filenames = filenames\n",
    "        s.batch_size = batch_size\n",
    "        s.num_epochs = num_epochs\n",
    "        s.read_threads = read_threads\n",
    "        s.seed = seed\n",
    "\n",
    "    def _read_and_decode(s, filename_queue):\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, example = reader.read(filename_queue)\n",
    "        features={\n",
    "            'inputs': tf.FixedLenFeature([N_SEQ * N_INPUTS], tf.float32),\n",
    "            'targets': tf.FixedLenFeature([N_SEQ * N_OUTPUTS], tf.float32),\n",
    "        }\n",
    "        example = tf.parse_single_example(example, features)\n",
    "        inputs = tf.reshape(example['inputs'], [N_SEQ, N_INPUTS])\n",
    "        targets = tf.reshape(example['targets'], [N_SEQ, N_OUTPUTS])\n",
    "        return inputs, targets\n",
    "\n",
    "    def __call__(s):\n",
    "        with tf.name_scope('input_queues'):\n",
    "            #with tf.device(\"/cpu:0\"):  # Input queues are on CPU.\n",
    "            filenames = [os.path.join(DATA_DIR, filename) for filename in s.filenames]\n",
    "            filename_queue = tf.train.string_input_producer(filenames, s.num_epochs, shuffle=True)\n",
    "\n",
    "            examples = [s._read_and_decode(filename_queue) for _ in range(s.read_threads)]\n",
    "\n",
    "            # Shuffle examples.\n",
    "            if True:\n",
    "                min_after_dequeue = 10 #10000\n",
    "                capacity = min_after_dequeue + (s.read_threads + 2) * s.batch_size\n",
    "                input_batch, target_batch = tf.train.shuffle_batch_join(\n",
    "                        examples, batch_size=s.batch_size, seed=s.seed, capacity=capacity,\n",
    "                        min_after_dequeue=min_after_dequeue, allow_smaller_final_batch=True)\n",
    "            else:\n",
    "                assert s.read_threads == 1\n",
    "                input_batch, target_batch = examples[0]\n",
    "            return {'inputs': input_batch}, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make one pass over the dataset to make sure the input pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = DataLoader(['train.tfrecords'])()[0]['inputs']\n",
    "\n",
    "sess = tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "idx = 0\n",
    "training_data = np.empty((N_SEQ, TRAINING_LEN-N_OUTPUTS))\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        training_data[:, idx:idx+N_INPUTS] = sess.run(inputs)\n",
    "        idx += 1\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done: {} steps'.format(idx))\n",
    "finally:\n",
    "    coord.request_stop()\n",
    "\n",
    "coord.join(threads)\n",
    "sess.close()\n",
    "\n",
    "#np.testing.assert_allclose(training_data, data.iloc[:, :TRAINING_LEN-N_OUTPUTS])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}