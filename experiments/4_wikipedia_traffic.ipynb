{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Traffic\n",
    "\n",
    "This notebook aims at modeling user traffic on [Wikipedia](https://wikipedia.org) using a recurrent graph convolutional neural network.\n",
    "\n",
    "Goal: anomaly detection. Can be used to detect events in the real world. Other applications:\n",
    "* intrusion detection on telecomunnication networks,\n",
    "* anomaly detection on energy networks,\n",
    "* accident detection on transporation networks.\n",
    "\n",
    "Events: Super Bowl, Academy Awards, Grammy, Miss Universe, Golden Globe. Mostly December-February.\n",
    "Missed: Charlie Hebdo, Ebola\n",
    "\n",
    "Network is very large: 5M nodes, 300M edges. Downsampling ideas:\n",
    "* Choose a category, e.g. science.\n",
    "* Take most active ones.\n",
    "* Concatenate in modules / communities / super-nodes.\n",
    "\n",
    "Raw data\n",
    "* [Wikimedia SQL dumps](https://dumps.wikimedia.org/enwiki/), to construct the hyperlink graph.\n",
    "    * Network size: 5M nodes, 300M edges.\n",
    "* [Pagecounts](https://dumps.wikimedia.org/other/pagecounts-all-sites/) as activations on the graph.\n",
    "    * Data from 2014-09-23 0h to 2015-06-05 22h.\n",
    "    * 6142 hours in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graph_tool.all as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "WIKI_RAW = os.environ.get('WIKI_RAW')  # Downloaded from dumps.wikimedia.org.\n",
    "WIKI_CLEAN = os.environ.get('WIKI_CLEAN')  # Processed by Kirell Benzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Hyperlink graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt.load_graph(os.path.join(WIKI_CLEAN, 'enwiki-20150403-graph.gt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.is_directed()\n",
    "#g.set_directed(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.2e} vertices'.format(g.num_vertices()))\n",
    "print('{:.2e} edges'.format(g.num_edges()))\n",
    "\n",
    "g.list_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "page_title = g.vertex_properties['page_title'][idx]\n",
    "page_id = g.vertex_properties['page_id'][idx]\n",
    "print('{}: {}'.format(page_id, page_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = gt.vertex_hist(g, 'total')\n",
    "plt.loglog(hist[1][:-1], hist[0])\n",
    "plt.xlabel('#edges')\n",
    "plt.ylabel('#nodes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too large to be drawn in full.\n",
    "#gt.sfdp_layout\n",
    "#gt.graph_draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uninteresting pages.\n",
    "#g.set_vertex_filter()\n",
    "#g.remove_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = gt.adjacency(g)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pages\n",
    "\n",
    "A lot of pages in `pagecounts` are redirections to actual pages. We need to merge the hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(WIKI_CLEAN, 'enwiki-20150403-page-redirect.csv.gz')\n",
    "redirect = pd.read_csv(filepath, compression='gzip', sep='|', encoding='utf-8', quoting=3, index_col=1)\n",
    "\n",
    "redirect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert len(redirect) == len(redirect['page_id'].unique())\n",
    "print('{:.2e} unique pages, {:.2e} pages including redirections'.format(\n",
    "        len(redirect['fix_page_id'].unique()),\n",
    "        len(redirect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect.loc[page_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2title(page_id):\n",
    "    return redirect.at[page_id, 'fix_page_title']\n",
    "    #return redirect[redirect['page_id'] == page_id]['fix_page_title'].values[0]\n",
    "id2title(330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_title(string):\n",
    "\n",
    "    def find(page_title, string):\n",
    "        try:\n",
    "            return string.lower() in page_title.lower()\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    #b = redirect['fix_page_title'].apply(find, string=string)\n",
    "    b = redirect['page_title'].apply(find, string=string)\n",
    "    #return redirect[b]\n",
    "    return redirect[b & (redirect['is_redirect'] == 0)]\n",
    "\n",
    "#find_in_title('ebola')\n",
    "find_in_title('zirka')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Page views / counts\n",
    "\n",
    "Graph has 4M nodes but lot of pages are not seen much. `signal_500.h5` lists only 118k pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kirell's signal which includes views when greater than 500.\n",
    "filepath = os.path.join(WIKI_CLEAN, 'signal_500.h5')\n",
    "signal = pd.read_hdf(filepath, 'data')\n",
    "signal['count_views'].plot(kind='hist', logy=True)\n",
    "print(len(signal), len(signal['page_id'].unique()), len(signal['layer'].unique()), signal['count_views'].max())\n",
    "signal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/wikipedia/activations_all.h5'\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    activations = pd.read_hdf(filepath, 'activations')\n",
    "\n",
    "else:\n",
    "    START = datetime.datetime(2014, 9, 23, 2)\n",
    "    #END = datetime.datetime(2014, 9, 24, 2)\n",
    "    END = datetime.datetime(2015, 6, 5, 20)\n",
    "\n",
    "    activations = pd.DataFrame(columns=pd.date_range(START, END, freq='H'))\n",
    "\n",
    "    folder = os.path.join(WIKI_CLEAN, 'pagecounts_clean')\n",
    "    for date in tqdm_notebook(activations.columns):\n",
    "        filename = 'pagecounts-{:4d}{:02d}{:02d}-{:02d}0000.csv.gz'.format(date.year, date.month, date.day, date.hour)\n",
    "        filename = os.path.join(folder, filename)\n",
    "        pagecounts = pd.read_csv(filename, compression='gzip', index_col=0, squeeze=True)\n",
    "        #print(len(pagecounts), filename)\n",
    "        print(date)\n",
    "        activations[date] = pagecounts\n",
    "        activations[date] = activations[date].fillna(0).astype(np.int32)\n",
    "\n",
    "    activations.to_hdf(filepath, 'activations')\n",
    "\n",
    "print(activations.shape)\n",
    "activations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predictable fluctuations with unpredictable spikes. Those are outliers.\n",
    "* Anomalies should be outliers persisting for many hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_id = 40817806\n",
    "page_id = 25\n",
    "title = '{} ({})'.format(id2title(page_id), page_id)\n",
    "activations.loc[page_id].plot(title=title)\n",
    "plt.ylabel('#hits per hour');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activations.plot(kind='hist', logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_REMOVE = [15580374, 42727860] # page ids to remove (Main page, Undefined)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}