{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Traffic\n",
    "\n",
    "This notebook aims at modeling user traffic on [Wikipedia](https://wikipedia.org) using a recurrent graph convolutional neural network.\n",
    "\n",
    "Goal: anomaly detection. Can be used to detect events in the real world. Other applications:\n",
    "* intrusion detection on telecomunnication networks,\n",
    "* anomaly detection on energy networks,\n",
    "* accident detection on transporation networks.\n",
    "\n",
    "Events: Super Bowl, Academy Awards, Grammy, Miss Universe, Golden Globe. Mostly December-February.\n",
    "Missed: Charlie Hebdo, Ebola\n",
    "\n",
    "Network is very large: 5M nodes, 300M edges. Downsampling ideas:\n",
    "* Choose a category, e.g. science.\n",
    "* Take most active ones.\n",
    "* Concatenate in modules / communities / super-nodes.\n",
    "\n",
    "Raw data\n",
    "* [Wikimedia SQL dumps](https://dumps.wikimedia.org/enwiki/), to construct the hyperlink graph.\n",
    "    * Network size: 5M nodes, 300M edges.\n",
    "* [Pagecounts](https://dumps.wikimedia.org/other/pagecounts-all-sites/) as activations on the graph.\n",
    "    * Data from 2014-09-23 0h to 2015-06-05 22h.\n",
    "    * 6142 hours in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graph_tool.all as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "WIKI_RAW = os.environ.get('WIKI_RAW')  # Downloaded from dumps.wikimedia.org.\n",
    "WIKI_CLEAN = os.environ.get('WIKI_CLEAN')  # Processed by Kirell Benzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "plt.rcParams['agg.path.chunksize'] = 10000  # OverflowError when plotting large series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Hyperlink graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt.load_graph(os.path.join(WIKI_CLEAN, 'enwiki-20150403-graph.gt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.is_directed()\n",
    "#g.set_directed(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.2e} vertices'.format(g.num_vertices()))\n",
    "print('{:.2e} edges'.format(g.num_edges()))\n",
    "\n",
    "g.list_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "page_title = g.vertex_properties['page_title'][idx]\n",
    "page_id = g.vertex_properties['page_id'][idx]\n",
    "print('{}: {}'.format(page_id, page_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = gt.vertex_hist(g, 'total')\n",
    "plt.loglog(hist[1][:-1], hist[0])\n",
    "plt.xlabel('#edges')\n",
    "plt.ylabel('#nodes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too large to be drawn in full.\n",
    "#gt.sfdp_layout\n",
    "#gt.graph_draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uninteresting pages.\n",
    "#g.set_vertex_filter()\n",
    "#g.remove_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = gt.adjacency(g)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pages\n",
    "\n",
    "A lot of pages in `pagecounts` are redirections to actual pages. We need to merge the hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(WIKI_CLEAN, 'enwiki-20150403-page-redirect.csv.gz')\n",
    "redirect = pd.read_csv(filepath, compression='gzip', sep='|', encoding='utf-8', quoting=3, index_col=1)\n",
    "\n",
    "redirect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert len(redirect) == len(redirect['page_id'].unique())\n",
    "print('{:.2e} unique pages, {:.2e} pages including redirections'.format(\n",
    "        len(redirect['fix_page_id'].unique()),\n",
    "        len(redirect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect.loc[page_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2title(page_id):\n",
    "    return redirect.at[page_id, 'fix_page_title']\n",
    "    #return redirect[redirect['page_id'] == page_id]['fix_page_title'].values[0]\n",
    "id2title(330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_title(string):\n",
    "\n",
    "    def find(page_title, string):\n",
    "        try:\n",
    "            return string.lower() in page_title.lower()\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    #b = redirect['fix_page_title'].apply(find, string=string)\n",
    "    b = redirect['page_title'].apply(find, string=string)\n",
    "    #return redirect[b]\n",
    "    return redirect[b & (redirect['is_redirect'] == 0)]\n",
    "\n",
    "find_in_title('ebola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Page views / counts\n",
    "\n",
    "Graph has 4M nodes but lot of pages are not seen much. `signal_500.h5` lists only 118k pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kirell's signal which includes views when greater than 500.\n",
    "filepath = os.path.join(WIKI_CLEAN, 'signal_500.h5')\n",
    "signal = pd.read_hdf(filepath, 'data')\n",
    "signal['count_views'].plot(kind='hist', logy=True)\n",
    "print(len(signal), len(signal['page_id'].unique()), len(signal['layer'].unique()), signal['count_views'].max())\n",
    "signal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagecounts(date):\n",
    "    filename = 'pagecounts-{:4d}{:02d}{:02d}-{:02d}0000.csv.gz'.format(date.year, date.month, date.day, date.hour)\n",
    "    filepath = os.path.join('..', 'data', 'wikipedia', 'pagecounts_clean', filename)\n",
    "    return pd.read_csv(filepath, compression='gzip', index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime.datetime(2014, 9, 23, 2)\n",
    "END = datetime.datetime(2014, 9, 24, 3)\n",
    "END = datetime.datetime(2015, 6, 5, 20)\n",
    "dates = pd.date_range(START, END, freq='H')\n",
    "\n",
    "activations_tot = pd.Series(\n",
    "    data=0,\n",
    "    index=g.vp['page_id'].get_array(),\n",
    "    dtype=np.int64\n",
    ")\n",
    "\n",
    "for date in tqdm_notebook(dates):\n",
    "    pagecounts = get_pagecounts(date)\n",
    "    activations_tot += pagecounts.reindex(activations_tot.index).fillna(0).astype(np.int32)\n",
    "\n",
    "print(activations_tot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The largest is the main page.\n",
    "plt.semilogy(np.sort(activations_tot.values)[::-1])\n",
    "\n",
    "main_page = activations_tot.argmax()\n",
    "print('{} ({}): {:.2e} views in total'.format(id2title(main_page), main_page, activations_tot[main_page]))\n",
    "\n",
    "print('{:.2e} views in total'.format(activations_tot.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power law.\n",
    "activations_tot.drop(main_page).plot(kind='hist', logy=True, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_AVG_VIEWS = 100\n",
    "\n",
    "keep = activations_tot.index[activations_tot >= MIN_AVG_VIEWS * len(dates)]\n",
    "print('{} pages have more than {} views in total ({:.0f} per hour on average)'.format(\n",
    "    len(keep), MIN_AVG_VIEWS * len(dates), MIN_AVG_VIEWS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = pd.DataFrame(\n",
    "    data=0,\n",
    "    index=keep,\n",
    "    columns=dates,\n",
    "    dtype=np.int32\n",
    ")\n",
    "\n",
    "for date in tqdm_notebook(dates):\n",
    "    pagecounts = get_pagecounts(date)\n",
    "    activations[date] = pagecounts.reindex(activations.index).fillna(0).astype(np.int32)\n",
    "\n",
    "filepath = os.path.join('..', 'data', 'wikipedia', 'activations_{}.h5'.format(MIN_AVG_VIEWS))\n",
    "activations.to_hdf(filepath, 'activations')\n",
    "\n",
    "print('activations: {} x {} = {}'.format(*activations.shape, activations.size))\n",
    "ipd.display(activations.head())\n",
    "ipd.display(activations.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predictable fluctuations with unpredictable spikes. Those are outliers.\n",
    "* Anomalies should be outliers persisting for many hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = pd.read_hdf(filepath, 'activations')\n",
    "\n",
    "DROP = [\n",
    "    15580374,  # Main page draws ~10% traffic\n",
    "    42727860,  # Undefined has the largest peaks of traffic while being inactive after 2014-10\n",
    "#   8063851,   # Feynman point has a very large traffic peak which is probably an error.\n",
    "#   2697304,   # Gold_as_an_investment has many traffic peaks.\n",
    "]\n",
    "activations.drop(DROP, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max of {0} views at page id {2} and time {1}'.format(\n",
    "    activations.unstack().max(), *activations.unstack().argmax())) \n",
    "plt.plot(activations.values.reshape(-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(activations.values.reshape(-1), bins=100, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events.\n",
    "page_id = 40817806  # Ebola\n",
    "page_id = 44635     # Grammy\n",
    "page_id = 150340    # Miss Universe\n",
    "page_id = 27718     # Super Bowl\n",
    "#page_id = 324       # Academy Awards\n",
    "#page_id = 44969225  # Charlie Hebdo shooting\n",
    "#page_id = 2251390   # Charlie Hebdo\n",
    "\n",
    "# Remarkable things.\n",
    "#page_id = 25\n",
    "#page_id = 15580374  # Main Page --> largest traffic (~10%)\n",
    "#page_id = 42727860  # Undefined --> hits only before mid-oct 2014\n",
    "#page_id = 670       # Alphabet --> strange drop\n",
    "#page_id = 8063851   # Shall distinguish outliers (counting errors?) from real events\n",
    "#page_id = 2697304   # Lots of peaks --> correlated with fluctuations on market?\n",
    "\n",
    "page_title = id2title(page_id)\n",
    "activations.loc[page_id].plot(title='{} ({})'.format(page_title, page_id), logy=True)\n",
    "plt.ylabel('#views per hour');\n",
    "#plt.savefig('{}_{}.png'.format(page_id, page_title.lower()), dpi=300)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}